%% example from http://shelah.logic.at/mybib.bib

@article{yu2017devil,
	title={The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching},
	author={Yu, Qian and Chang, Xiaobin and Song, Yi-Zhe and Xiang, Tao and Hospedales, Timothy M},
	journal={arXiv preprint arXiv:1711.08106},
	year={2017}
}

@article{zheng2017person,
	archivePrefix = {arXiv},
	author = {Zheng, Liang and Zhang, Hengheng and Sun, Shaoyan and Chandraker, Manmohan and Tian, Qi},
	journal = {arXiv preprint arXiv:1604.02531},
	title = {{Person Re-identification in the Wild}},
	year = {2017}
}

@article{zheng2016person,
	title={Person re-identification: Past, present and future},
	author={Zheng, Liang and Yang, Yi and Hauptmann, Alexander G},
	journal={arXiv preprint arXiv:1610.02984},
	year={2016}
}

@article{zhang2017align,
	author    = {Xuan Zhang and
			Hao Luo and
			Xing Fan and
			Weilai Xiang and
			Yixiao Sun and
			Qiqi Xiao and
			Wei Jiang and
			Chi Zhang and
			Jian Sun},
	title     = {AlignedReID: Surpassing Human-Level Performance in Person Re-Identification},
	journal   = {CoRR},
	volume    = {abs/1711.08184},
	year      = {2017},
	archivePrefix = {arXiv},
	
	timestamp = {Sun, 03 Dec 2017 12:38:15 +0100},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{latent,
	author    = {Dangwei Li and
			Xiaotang Chen and
			Zhang Zhang and
			Kaiqi Huang},
	title     = {Learning Deep Context-aware Features over Body and Latent Parts for
			Person Re-identification},
	journal   = {CoRR},
	volume    = {abs/1710.06555},
	year      = {2017},
	archivePrefix = {arXiv},
	
	timestamp = {Wed, 01 Nov 2017 19:05:42 +0100},

	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{varior2016gated,
	title={Gated siamese convolutional neural network architecture for human re-identification},
	author={Varior, Rahul Rama and Haloi, Mrinal and Wang, Gang},
	booktitle={European Conference on Computer Vision},
	pages={791--808},
	year={2016},
	organization={Springer}
}
@article{hermans2017defense,
	title={In defense of the triplet loss for person re-identification},
	author={Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
	journal={arXiv preprint arXiv:1703.07737},
	year={2017}
}
@article{zheng2017unlabeled,
	title={Unlabeled samples generated by gan improve the person re-identification baseline in vitro},
	author={Zheng, Zhedong and Zheng, Liang and Yang, Yi},
	journal={arXiv preprint arXiv:1701.07717},
	volume={3},
	year={2017}
}
@article{qian2017pose,
	title={Pose-Normalized Image Generation for Person Re-identification},
	author={Qian, Xuelin and Fu, Yanwei and Wang, Wenxuan and Xiang, Tao and Wu, Yang and Jiang, Yu-Gang and Xue, Xiangyang},
	journal={arXiv preprint arXiv:1712.02225},
	year={2017}
}
@article{liu2017video,
	title={Video-based person re-identification with accumulative motion context},
	author={Liu, Hao and Jie, Zequn and Jayashree, Karlekar and Qi, Meibin and Jiang, Jianguo and Yan, Shuicheng and Feng, Jiashi},
	journal={IEEE Transactions on Circuits and Systems for Video Technology},
	year={2017},
	publisher={IEEE}
}
@inproceedings{liu2017quality,
	title={Quality aware network for set to set recognition},
	author={Liu, Yu and Yan, Junjie and Ouyang, Wanli},
	booktitle={Proc. IEEE Int. Conf. Comput. Vision Pattern Recognit.},
	pages={5790--5799},
	year={2017}
}
@article{reciprocal,
	author    = {Zhun Zhong and
			Liang Zheng and
			Donglin Cao and
			Shaozi Li},
	title     = {Re-ranking Person Re-identification with k-reciprocal Encoding},
	journal   = {CoRR},
	volume    = {abs/1701.08398},
	year      = {2017},

	archivePrefix = {arXiv},
	
	timestamp = {Wed, 07 Jun 2017 14:40:10 +0200},

	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{liu2017hydraplus,
	title={Hydraplus-net: Attentive deep features for pedestrian analysis},
	author={Liu, Xihui and Zhao, Haiyu and Tian, Maoqing and Sheng, Lu and Shao, Jing and Yi, Shuai and Yan, Junjie and Wang, Xiaogang},
	journal={arXiv preprint arXiv:1709.09930},
	year={2017}
}

@article{guo2018multilevel,
abstract = {Person Re-Identification (ReID) requires comparing two images of person captured under different conditions. Existing work based on neural networks often computes the similarity of feature maps from one single convolutional layer. In this work, we propose an efficient, end-to-end fully convolutional Siamese network that computes the similarities at multiple levels. We demonstrate that multi-level similarity can improve the accuracy considerably using low-complexity network structures in ReID problem. Specifically, first, we use several convolutional layers to extract the features of two input images. Then, we propose Convolution Similarity Network to compute the similarity score maps for the inputs. We use spatial transformer networks (STNs) to determine spatial attention. We propose to apply efficient depth-wise convolution to compute the similarity. The proposed Convolution Similarity Networks can be inserted into different convolutional layers to extract visual similarities at different levels. Furthermore, we use an improved ranking loss to further improve the performance. Our work is the first to propose to compute visual similarities at low, middle and high levels for ReID. With extensive experiments and analysis, we demonstrate that our system, compact yet effective, can achieve competitive results with much smaller model size and computational complexity.},
archivePrefix = {arXiv},
author = {Guo, Yiluan and Cheung, Ngai-Man},
file = {:home/xinglu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/CVPR18{\_}re-id/CVPR2018{\_}Efficient and Deep Person Re-Identification using Multi-Level Similarity.pdf:pdf},
title = {{Efficient and Deep Person Re-Identification using Multi-Level Similarity}},
volume = {1},
year = {2018},
journal={arXiv preprint arXiv:1803.11353},
}

@article{chang2018factor,
abstract = {Key to effective person re-identification (Re-ID) is modelling discriminative and view-invariant factors of person appearance at both high and low semantic levels. Recently developed deep Re-ID models either learn a holistic single semantic level feature representation and/or require laborious human annotation of these factors as attributes. We propose Multi-Level Factorisation Net (MLFN), a novel network architecture that factorises the visual appearance of a person into latent discriminative factors at multiple semantic levels without manual annotation. MLFN is composed of multiple stacked blocks. Each block contains multiple factor modules to model latent factors at a specific level, and factor selection modules that dynamically select the factor modules to interpret the content of each input image. The outputs of the factor selection modules also provide a compact latent factor descriptor that is complementary to the conventional deeply learned features. MLFN achieves state-of-the-art results on three Re-ID datasets, as well as compelling results on the general object categorisation CIFAR-100 dataset.},
archivePrefix = {arXiv},
author = {Chang, Xiaobin and Hospedales, Timothy M. and Xiang, Tao},

file = {:home/xinglu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/CVPR18{\_}re-id/CVPR2018{\_}Multi-Level Factorisation Net for Person Re-Identification.pdf:pdf},
month = {mar},
number = {I},
title = {{Multi-Level Factorisation Net for Person Re-Identification}},
volume = {2},
year = {2018},
journal={arXiv preprint arXiv:1803.09132},
}

@inproceedings{zhao2017spindle,
	title={Spindle net: Person re-identification with human body region guided feature decomposition and fusion},
	author={Zhao, Haiyu and Tian, Maoqing and Sun, Shuyang and Shao, Jing and Yan, Junjie and Yi, Shuai and Wang, Xiaogang and Tang, Xiaoou},
	booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages={1077--1085},
	year={2017}
}

@article{glad,
	author    = {Longhui Wei and
			Shiliang Zhang and
			Hantao Yao and
			Wen Gao and
			Qi Tian},
	title     = {GLAD: Global-Local-Alignment Descriptor for Pedestrian Retrieval},
	journal   = {CoRR},
	volume    = {abs/1709.04329},
	year      = {2017},

	archivePrefix = {arXiv},
	
	timestamp = {Thu, 05 Oct 2017 09:43:05 +0200},

	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{schroff2015facenet,
  title={Facenet: A unified embedding for face recognition and clustering},
  author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={815--823},
  year={2015}
}

@inproceedings{chen2017beyond,
  title={Beyond triplet loss: a deep quadruplet network for person re-identification},
  author={Chen, Weihua and Chen, Xiaotang and Zhang, Jianguo and Huang, Kaiqi},
  booktitle={Proc. CVPR},
  volume={2},
  year={2017}
}

@article{xiao2017margin,
  title={Margin Sample Mining Loss: A Deep Learning Based Method for Person Re-identification},
  author={Xiao, Qiqi and Luo, Hao and Zhang, Chi},
  journal={arXiv preprint arXiv:1710.00478},
  year={2017}
}

@article{yaqing2016semantics,
abstract = {In this paper, we propose an end-to-end deep corre-spondence structure learning (DCSL) approach to address the cross-camera person-matching problem in the person re-identification task. The proposed DCSL approach captures the intrinsic structural information on persons by learning a semantics-aware image representation based on convolution-al neural networks, which adaptively learns dis-criminative features for person identification. Fur-thermore, the proposed DCSL approach seeks to adaptively learn a hierarchical data-driven fea-ture matching function which outputs the match-ing correspondence results between the learned semantics-aware image representations for a person pair. Finally, we set up a unified end-to-end deep learning scheme to jointly optimize the processes of semantics-aware image representation learning and cross-person correspondence structure learn-ing, leading to more reliable and robust person re-identification results in complicated scenarios. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach a-gainst the state-of-the-art approaches.},
author = {Yaqing, Zhang and Li, Xi and Zhao, Liming and Zhongfei, Zhang},
file = {:home/xinglu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yaqing et al. - 2016 - Semantics-aware deep correspondence structure learning for robust person re-identification.pdf:pdf},
isbn = {978-1-57735-770-4},
issn = {10450823},
journal = {IJCAI Int. Jt. Conf. Artif. Intell.},
pages = {3545--3551},
title = {{Semantics-aware deep correspondence structure learning for robust person re-identification}},
volume = {2016-Janua},
year = {2016}
}

@article{zhao2017part,
abstract = {In this paper, we address the problem of person re-identification, which refers to associating the persons captured from different cameras. We propose a simple yet effective human part-aligned representation for handling the body part misalignment problem. Our approach decomposes the human body into regions (parts) which are discriminative for person matching, accordingly computes the representations over the regions, and aggregates the similarities computed between the corresponding regions of a pair of probe and gallery images as the overall matching score. Our formulation, inspired by attention models, is a deep neural network modeling the three steps together, which is learnt through minimizing the triplet loss function without requiring body part labeling information. Unlike most existing deep learning algorithms that learn a global or spatial partition-based local representation, our approach performs human body partition, and thus is more robust to pose changes and various human spatial distributions in the person bounding box. Our approach shows state-of-the-art results over standard datasets, Market-{\$}1501{\$}, CUHK{\$}03{\$}, CUHK{\$}01{\$} and VIPeR.},
archivePrefix = {arXiv},
author = {Zhao, Liming and Li, Xi and Wang, Jingdong and Zhuang, Yueting},

file = {:home/xinglu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2017 - Deeply-Learned Part-Aligned Representations for Person Re-Identification.pdf:pdf},
month = {jul},
pages = {3219--3228},
title = {{Deeply-Learned Part-Aligned Representations for Person Re-Identification}},

year = {2017},
journal={arXiv preprint arXiv:1707.07256}
}

@inproceedings{deng2009imagenet,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called {\&}{\#}x201C;ImageNet{\&}{\#}x201D;, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {{Jia Deng} and {Wei Dong} and Socher, R. and {Li-Jia Li} and {Kai Li} and {Li Fei-Fei}},
booktitle = {2009 IEEE Conf. Comput. Vis. Pattern Recognit.},

isbn = {978-1-4244-3992-8},
issn = {1063-6919},
organization = {IEEE},
pages = {248--255},
pmid = {21914436},
title = {{ImageNet: A large-scale hierarchical image database}},

year = {2009}
}

@inproceedings{szegedy2015going,
abstract = {We propose a deep convolutional neural network ar-chitecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the compu-tational budget constant. To optimize quality, the architec-tural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in-carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proc. IEEE Conf. Comput. Vis. Pattern Recognit.},
file = {:home/xinglu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - Unknown - Going Deeper with Convolutions.pdf:pdf},
pages = {1--9},
title = {{Going deeper with convolutions}},

year = {2015}
}

@article{hu2017senet,
abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, much existing work has shown the benefits of enhancing spatial encoding. In this work, we focus on channels and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at slight computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251{\%}, achieving a 25{\%} relative improvement over the winning entry of 2016.},
archivePrefix = {arXiv},
author = {Hu, Jie and Shen, Li and Sun, Gang},
file = {:home/xinglu/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu, Shen, Sun - 2017 - Squeeze-and-Excitation Networks.pdf:pdf},
month = {sep},
title = {{Squeeze-and-Excitation Networks}},
year = {2017}, 
journal={arXiv preprint arXiv:1709.01507}
}

@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European Conference on Computer Vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}


@Article{kumar2017smart,
  author        = {B. G. Vijay Kumar and Ben Harwood and Gustavo Carneiro and Ian D. Reid and Tom Drummond},
  title         = {Smart Mining for Deep Metric Learning},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1704.01285},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},

  
  timestamp     = {Wed, 07 Jun 2017 14:40:25 +0200},

}

@InProceedings{smirnov2017doppelganger,
  author    = {Smirnov, Evgeny and Melnikov, Aleksandr and Novoselov, Sergey and Luckyanets, Eugene and Lavrentyeva, Galina},
  title     = {Doppelganger Mining for Face Representation Learning},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2017},
  pages     = {1916--1923},
}

@InProceedings{li2014deepreid,
  author    = {Li, Wei and Zhao, Rui and Xiao, Tong and Wang, Xiaogang},
  title     = {Deepreid: Deep filter pairing neural network for person re-identification},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2014},
  pages     = {152--159},
}

@InProceedings{li2013locally,
  author       = {Li, Wei and Wang, Xiaogang},
  title        = {Locally aligned feature transforms across views},
  booktitle    = {Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on},
  year         = {2013},
  pages        = {3594--3601},
  organization = {IEEE},
}
@InProceedings{zheng2015scalable,
  author    = {Zheng, Liang and Shen, Liyue and Tian, Lu and Wang, Shengjin and Wang, Jingdong and Tian, Qi},
  title     = {Scalable person re-identification: A benchmark},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  year      = {2015},
  pages     = {1116--1124},
}

@InProceedings{ristani2016MTMC,
  author    = {Ristani, Ergys and Solera, Francesco and Zou, Roger and Cucchiara, Rita and Tomasi, Carlo},
  title     = {Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking},
  booktitle = {European Conference on Computer Vision workshop on Benchmarking Multi-Target Tracking},
  year      = {2016},
}
@InProceedings{gray2007evaluating,
  author       = {Gray, Douglas and Brennan, Shane and Tao, Hai},
  title        = {Evaluating appearance models for recognition, reacquisition, and tracking},
  booktitle    = {Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)},
  year         = {2007},
  volume       = {3},
  pages        = {1--7},
  organization = {Citeseer},
}
@incollection{prid450s,
  title={Mahalanobis distance learning for person re-identification},
  author={Roth, Peter M and Hirzer, Martin and Koestinger, Martin and Beleznai, Csaba and Bischof, Horst},
  booktitle={Person Re-Identification},
  pages={247--267},
  year={2014},
  publisher={Springer}
}
@article{ilids,
  title={i-LIDS multiple camera tracking scenario definition},
  author={Office, U H},
  year={2008},
}
@inproceedings{prid2011,
  title={Person re-identification by descriptive and discriminative classification},
  author={Hirzer, Martin and Beleznai, Csaba and Roth, Peter M. and Bischof, Horst},
  booktitle={Scandinavian Conference on Image Analysis},
  pages={91-102},
  year={2011},
}
@inproceedings{itml,
  title={Information-theoretic metric learning},
  author={Davis, Jason V. and Kulis, Brian and Jain, Prateek and Sra, Suvrit and Dhillon, Inderjit S.},
  booktitle={ICML},
  pages={209-216},
  year={2007},
}
@INPROCEEDINGS{lmnn,
	title={Distance metric learning for large margin nearest neighbor classification},
	author={Weinberger, Kilian Q and Saul, Lawrence K},
	journal={Journal of Machine Learning Research},
	volume={10},
	pages={207--244},
	year={2009}
}
@INPROCEEDINGS{kissme,
	title={Large scale metric learning from equivalence constraints},
	author={Koestinger, Martin and Hirzer, Martin and Wohlhart, Paul and Roth, Peter M and Bischof, Horst},
	booktitle={CVPR},
	pages={2288--2295},
	year={2012},
	organization={IEEE}
}
@INPROCEEDINGS{lstm,
	title={A siamese long short-term memory architecture for human re-identification},
	author={Varior, Rahul Rama and Shuai, Bing and Lu, Jiwen and Xu, Dong and Wang, Gang},
	booktitle={ECCV},
	pages={135--153},
	year={2016},
	organization={Springer}
}

@INPROCEEDINGS{improveddl,
	title={An improved deep learning architecture for person re-identification},
	author={Ahmed, Ejaz and Jones, Michael and Marks, Tim K},
	booktitle={CVPR},
	pages={3908--3916},
	year={2015}
}

@INPROCEEDINGS{lsscdl,
	title={Sample-specific svm learning for person re-identification},
	author={Zhang, Ying and Li, Baohua and Lu, Huchuan and Irie, Atshushi and Ruan, Xiang},
	booktitle={CVPR},
	pages={1278--1287},
	year={2016}
}
@INPROCEEDINGS{xqda,
	title={Person re-identification by local maximal occurrence representation and metric learning},
	author={Liao, Shengcai and Hu, Yang and Zhu, Xiangyu and Li, Stan Z},
	booktitle={CVPR},
	pages={2197--2206},
	year={2015}
}
@INPROCEEDINGS{Alpher41,
	AUTHOR = "Li, Wei and Zhu, Xiatian and Gong, Shaogang",
	TITLE = "Person Re-Identification by Deep Joint Learning of Multi-Loss Classification",
	BOOKTITLE = "IJCAI",
	PAGES = "2194--2200",
	YEAR = {2017}
}

@inproceedings{mtd,
  title={A Multi-task Deep Network for Person Re-identification},
  author={Chen, Weihua and Chen, Xiaotang and Zhang, Jianguo and Huang, Kaiqi},
  booktitle={AAAI},
  pages={3988-3994},
  year={2017},
}
@INPROCEEDINGS{sicir,
	title={Joint learning of single-image and cross-image representations for person re-identification},
	author={Wang, Faqiang and Zuo, Wangmeng and Lin, Liang and Zhang, David and Zhang, Lei},
	booktitle={CVPR},
	pages={1288--1296},
	year={2016}
}

@INPROCEEDINGS{fpnn,
	AUTHOR = "Li, Wei and Zhao, Rui and Xiao, Tong and Wang, Xiaogang",
	TITLE = "Deepreid: Deep filter pairing neural network for person re-identification",
	BOOKTITLE = "CVPR",
	PAGES = "152--159",
	YEAR = {2014}
}
@inproceedings{tcp,
  title={Person Re-identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function},
  author={Cheng, De and Gong, Yihong and Zhou, Sanping and Wang, Jinjun and Zheng, Nanning},
  booktitle={CVPR},
  pages={1335-1344},
  year={2016},
}
@ARTICLE{pie,
	title={Pose invariant embedding for deep person re-identification},
	author={Zheng, Liang and Huang, Yujia and Lu, Huchuan and Yang, Yi},
	journal={arXiv preprint arXiv:1701.07732},
	year={2017}
}
@INPROCEEDINGS{jlml,
	AUTHOR = "Li, Wei and Zhu, Xiatian and Gong, Shaogang",
	TITLE = "Person Re-Identification by Deep Joint Learning of Multi-Loss Classification",
	BOOKTITLE = "IJCAI",
	PAGES = "2194--2200",
	YEAR = {2017}
}

@article{mao2018multi,
  title={Multi-Channel Pyramid Person Matching Network for Person Re-Identification},
  author={Mao, Chaojie and Li, Yingming and Zhang, Yaqing and Zhang, Zhongfei and Li, Xi},
  journal={arXiv preprint arXiv:1803.02558},
  year={2018}
}

@article{ristani2018features,
  title={Features for Multi-Target Multi-Camera Tracking and Re-Identification},
  author={Ristani, Ergys and Tomasi, Carlo},
  journal={arXiv preprint arXiv:1803.10859},
  year={2018}
}

@article{barbosa2017looking,
  title={Looking beyond appearances: Synthetic training data for deep cnns in re-identification},
  author={I. B. Barbosa and M. Cristani and B. Caputo and A. Rognhaugen and T. Theoharis},
  journal={arXiv preprint arXiv:1701.03153},
  year={2017}
}

@article{zheng2017ped,
  title={Pedestrian alignment network for large-scale person re-identification},
  author={Z. Zheng, L. Zheng, and Y. Yang},
  journal={arXiv preprint arXiv:1707.00408},
  year={2017}
}
@inproceedings{zhou2017see,
  title={See the forest for the trees: Joint spatial and temporal recurrent neural networks for video-based person re-identification},
  author={Zhou, Zhen and Huang, Yan and Wang, Wei and Wang, Liang and Tan, Tieniu},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},
  pages={6776--6785},
  year={2017},
  organization={IEEE}
}

@article{xu2017jointly,
  title={Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification},
  author={Xu, Shuangjie and Cheng, Yu and Gu, Kang and Yang, Yang and Chang, Shiyu and Zhou, Pan},
  journal={arXiv preprint arXiv:1708.02286},
  year={2017}
}